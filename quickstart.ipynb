{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd235fb0",
   "metadata": {},
   "source": [
    "# Bias Buccaneers Image Recognition Challenge: Quickstart\n",
    "\n",
    "This notebook will introduce you to the data and describe a workflow to train and evaluate a baseline model on it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad57b0b7",
   "metadata": {},
   "source": [
    "## Initial Setup\n",
    "\n",
    "We start with loading the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b63b689f-2d55-4c3f-9ed4-40b188667339",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyring is skipped due to an exception: 'keyring.backends'\n",
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.11.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (588.3 MB)\n",
      "Collecting nb-black\n",
      "  Downloading nb_black-1.0.7.tar.gz (4.8 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting opt-einsum>=2.3.2\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Collecting libclang>=13.0.0\n",
      "  Using cached libclang-14.0.6-py2.py3-none-manylinux2010_x86_64.whl (14.1 MB)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.14.0)\n",
      "Collecting protobuf<3.20,>=3.9.2\n",
      "  Using cached protobuf-3.19.6-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.11.2)\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Using cached grpcio-1.50.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (0.2.0)\n",
      "Collecting gast<=0.4.0,>=0.2.1\n",
      "  Using cached gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting flatbuffers>=2.0\n",
      "  Using cached flatbuffers-22.10.26-py2.py3-none-any.whl (26 kB)\n",
      "Collecting absl-py>=1.0.0\n",
      "  Using cached absl_py-1.3.0-py3-none-any.whl (124 kB)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from tensorflow) (59.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (4.4.0)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting keras<2.12,>=2.11.0\n",
      "  Using cached keras-2.11.0-py2.py3-none-any.whl (1.7 MB)\n",
      "Collecting tensorboard<2.12,>=2.11\n",
      "  Using cached tensorboard-2.11.0-py3-none-any.whl (6.0 MB)\n",
      "Collecting tensorflow-estimator<2.12,>=2.11.0\n",
      "  Using cached tensorflow_estimator-2.11.0-py2.py3-none-any.whl (439 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Using cached tensorflow_io_gcs_filesystem-0.27.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)\n",
      "Requirement already satisfied: numpy>=1.20 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.21.6)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from tensorflow) (20.1)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Using cached termcolor-2.1.0-py3-none-any.whl (5.8 kB)\n",
      "Requirement already satisfied: ipython in /opt/conda/lib/python3.7/site-packages (from nb-black) (7.34.0)\n",
      "Collecting black>='19.3'\n",
      "  Downloading black-22.10.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.7/site-packages (from astunparse>=1.6.0->tensorflow) (0.34.2)\n",
      "Requirement already satisfied: typed-ast>=1.4.2 in /opt/conda/lib/python3.7/site-packages (from black>='19.3'->nb-black) (1.5.4)\n",
      "Collecting click>=8.0.0\n",
      "  Downloading click-8.1.3-py3-none-any.whl (96 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.6/96.6 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting pathspec>=0.9.0\n",
      "  Downloading pathspec-0.10.2-py3-none-any.whl (28 kB)\n",
      "Requirement already satisfied: tomli>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from black>='19.3'->nb-black) (2.0.1)\n",
      "Collecting mypy-extensions>=0.4.3\n",
      "  Downloading mypy_extensions-0.4.3-py2.py3-none-any.whl (4.5 kB)\n",
      "Requirement already satisfied: platformdirs>=2 in /opt/conda/lib/python3.7/site-packages (from black>='19.3'->nb-black) (2.5.4)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Using cached google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Using cached tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "Collecting werkzeug>=1.0.1\n",
      "  Using cached Werkzeug-2.2.2-py3-none-any.whl (232 kB)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Using cached tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Using cached Markdown-3.4.1-py3-none-any.whl (93 kB)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Using cached google_auth-2.14.1-py2.py3-none-any.whl (175 kB)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.28.1)\n",
      "Requirement already satisfied: backcall in /opt/conda/lib/python3.7/site-packages (from ipython->nb-black) (0.1.0)\n",
      "Requirement already satisfied: pygments in /opt/conda/lib/python3.7/site-packages (from ipython->nb-black) (2.13.0)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.7/site-packages (from ipython->nb-black) (0.18.1)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from ipython->nb-black) (3.0.3)\n",
      "Requirement already satisfied: matplotlib-inline in /opt/conda/lib/python3.7/site-packages (from ipython->nb-black) (0.1.6)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.7/site-packages (from ipython->nb-black) (4.4.1)\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.7/site-packages (from ipython->nb-black) (0.7.5)\n",
      "Requirement already satisfied: traitlets>=4.2 in /opt/conda/lib/python3.7/site-packages (from ipython->nb-black) (5.5.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.7/site-packages (from ipython->nb-black) (4.8.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->tensorflow) (2.4.6)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from click>=8.0.0->black>='19.3'->nb-black) (5.0.0)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (4.9)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Using cached cachetools-5.2.0-py3-none-any.whl (9.3 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/conda/lib/python3.7/site-packages (from jedi>=0.16->ipython->nb-black) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.7/site-packages (from pexpect>4.3->ipython->nb-black) (0.6.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.7/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->nb-black) (0.1.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (2.8)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (2022.9.24)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (1.26.12)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.7/site-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->click>=8.0.0->black>='19.3'->nb-black) (3.10.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (0.4.8)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Building wheels for collected packages: nb-black\n",
      "  Building wheel for nb-black (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for nb-black: filename=nb_black-1.0.7-py3-none-any.whl size=5297 sha256=6c2f526ca3fe6ae4f27d844b5afcaf965aa451f0b958f523f8f75bde9c7a36d3\n",
      "  Stored in directory: /root/.cache/pip/wheels/79/29/4e/a4405cee966d1019a65c9d6bb6cd1bba039cd9476476703645\n",
      "Successfully built nb-black\n",
      "Installing collected packages: tensorboard-plugin-wit, mypy-extensions, libclang, flatbuffers, werkzeug, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, pyasn1-modules, protobuf, pathspec, opt-einsum, oauthlib, keras, grpcio, gast, cachetools, astunparse, absl-py, requests-oauthlib, markdown, google-auth, click, google-auth-oauthlib, black, tensorboard, nb-black, tensorflow\n",
      "  Attempting uninstall: werkzeug\n",
      "    Found existing installation: Werkzeug 1.0.0\n",
      "    Uninstalling Werkzeug-1.0.0:\n",
      "      Successfully uninstalled Werkzeug-1.0.0\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.20.3\n",
      "    Uninstalling protobuf-3.20.3:\n",
      "      Successfully uninstalled protobuf-3.20.3\n",
      "  Attempting uninstall: click\n",
      "    Found existing installation: Click 7.0\n",
      "    Uninstalling Click-7.0:\n",
      "      Successfully uninstalled Click-7.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "sagemaker 2.116.0 requires importlib-metadata<5.0,>=1.4.0, but you have importlib-metadata 5.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed absl-py-1.3.0 astunparse-1.6.3 black-22.10.0 cachetools-5.2.0 click-8.1.3 flatbuffers-22.10.26 gast-0.4.0 google-auth-2.14.1 google-auth-oauthlib-0.4.6 grpcio-1.50.0 keras-2.11.0 libclang-14.0.6 markdown-3.4.1 mypy-extensions-0.4.3 nb-black-1.0.7 oauthlib-3.2.2 opt-einsum-3.3.0 pathspec-0.10.2 protobuf-3.19.6 pyasn1-modules-0.2.8 requests-oauthlib-1.3.1 tensorboard-2.11.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.11.0 tensorflow-estimator-2.11.0 tensorflow-io-gcs-filesystem-0.27.0 termcolor-2.1.0 werkzeug-2.2.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow nb-black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb417ab0-f6ed-429e-b12b-6d72b9c5ea5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2454f20e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import io\n",
    "import boto3\n",
    "import json\n",
    "import os\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "56fd2d93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as K\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536f1fd6",
   "metadata": {},
   "source": [
    "## Load the Data\n",
    "\n",
    "Make sure to download and uncompress the data (`data_bb1_img_recognition.zip`) in the folder you're working off of.\n",
    "\n",
    "We first load the file containing the labels, binarize labels of each of the three classes as a numpy array and store them as a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "82c29b35-fab6-4050-8a4e-8d2be3dc4d5e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define data location\n",
    "bucket = \"gmml-test\"\n",
    "LOADPATH = \"00_raw_bb1/train/\"\n",
    "data_key = LOADPATH + \"labels.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9676940a-bf9d-4989-a064-675d784d14f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "s3 = boto3.client(\"s3\")\n",
    "obj = s3.get_object(Bucket=bucket, Key=data_key)\n",
    "df = pd.read_csv(obj[\"Body\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0c742f6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extract data\n",
    "df_labeled = df[df[\"skin_tone\"].notna()]  # take only labeled data\n",
    "\n",
    "# Converting labels to np array\n",
    "cat = [\"skin_tone\", \"gender\", \"age\"]\n",
    "lbs = [LabelBinarizer() for i in range(3)]\n",
    "Y = []\n",
    "for i in range(3):\n",
    "    lab = lbs[i].fit_transform(df_labeled[cat[i]])\n",
    "    if lab.shape[1] == 1:\n",
    "        Y.append(np.hstack((1 - lab, lab)))\n",
    "    else:\n",
    "        Y.append(lab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb73277",
   "metadata": {},
   "source": [
    "We then load the images under the training set and convert them to numpy arrays. This may take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0f957a4f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading images\n",
      "Converting images to np array\n"
     ]
    }
   ],
   "source": [
    "# loading and converting data into np array\n",
    "print(\"Loading images\")\n",
    "s3 = boto3.resource(\"s3\")\n",
    "length = width = 64  # size for each input image, increase if you want\n",
    "nn = df_labeled.shape[0]\n",
    "all_imgs = [\n",
    "    image.load_img(\n",
    "        io.BytesIO(\n",
    "            s3.Object(bucket, LOADPATH + df_labeled.iloc[i][\"name\"])\n",
    "            .get()[\"Body\"]\n",
    "            .read()\n",
    "        ),\n",
    "        target_size=(length, width),\n",
    "    )\n",
    "    for i in range(nn)\n",
    "]\n",
    "\n",
    "print(\"Converting images to np array\")\n",
    "X = np.empty([nn, length, width, 3], dtype=float)\n",
    "for i in range(nn):\n",
    "    X[i, :] = image.img_to_array(all_imgs[i])\n",
    "X = K.applications.resnet50.preprocess_input(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf14ba8",
   "metadata": {},
   "source": [
    "## Specify the Model\n",
    "\n",
    "We define a single model class that is able train on the data in `X` and `Y` and predict outcomes for all three classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "40c55abe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PredictionModel:\n",
    "    def __init__(self, X, Y, idx):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.idx = idx\n",
    "        self.trainX, self.testX = X[idx[0], :], X[idx[1], :]\n",
    "        self.trainY, self.testY = [Y[i][idx[0], :] for i in range(3)], [\n",
    "            Y[i][idx[1], :] for i in range(3)\n",
    "        ]\n",
    "        self.cat = [\"skin_tone\", \"gender\", \"age\"]\n",
    "        self.loss = [\"categorical_crossentropy\" for i in range(3)]\n",
    "        self.metrics = [[\"accuracy\"] for i in range(3)]\n",
    "        self.models = [None] * 3\n",
    "\n",
    "    # train a model specific for a certain class index in self.cat\n",
    "    def fit(\n",
    "        self,\n",
    "        index,\n",
    "        model,\n",
    "        epochs=5,\n",
    "        batch_size=32,\n",
    "        save=False,\n",
    "        save_location=None,\n",
    "        verbose=1,\n",
    "    ):\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Training model for \" + self.cat[index])\n",
    "        model.add(K.layers.Dense(self.trainY[index].shape[1], activation=\"softmax\"))\n",
    "        model.compile(\n",
    "            loss=self.loss[index], optimizer=\"Adam\", metrics=self.metrics[index]\n",
    "        )\n",
    "        model.fit(\n",
    "            self.trainX,\n",
    "            self.trainY[index],\n",
    "            validation_data=(self.testX, self.testY[index]),\n",
    "            batch_size=batch_size,\n",
    "            epochs=epochs,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "        if save:\n",
    "            if os.path.exists(SAVEPATH) == False:\n",
    "                print(\"save location \" + SAVEPATH + \" did not exist. creating\")\n",
    "                os.makedirs(SAVEPATH)\n",
    "            SAVE_LOCATION = save_location + \"model_\" + cat[index] + \".h5\"\n",
    "            print(\"saving model at \" + SAVE_LOCATION)\n",
    "            model.save(SAVE_LOCATION)\n",
    "        self.models[index] = model\n",
    "\n",
    "    def predict(self, newX):\n",
    "        predictions = [model.predict(newX) for model in self.models]\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76ae9ef",
   "metadata": {},
   "source": [
    "## Initialize and Train a Model\n",
    "\n",
    "We now train a `PredictionModel` to predict the likely skin tone, gender, and age of an input image. This baseline model is initialize on imagenet weights and uses the ResNet50 architecture. We strongly recommend using a GPU to reduce training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "da29fedf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# function to initialize a model\n",
    "def initializeModel():\n",
    "    res_model = ResNet50(\n",
    "        include_top=False,\n",
    "        weights=\"imagenet\",\n",
    "        input_tensor=K.Input(shape=[length, width, 3]),\n",
    "    )\n",
    "\n",
    "    # freeze all but the last layer\n",
    "    for layer in res_model.layers[:143]:\n",
    "        layer.trainable = False\n",
    "    model = K.models.Sequential()\n",
    "    model.add(res_model)\n",
    "    model.add(K.layers.Flatten())\n",
    "    model.add(K.layers.BatchNormalization())\n",
    "    model.add(K.layers.Dense(256, activation=\"relu\"))\n",
    "    model.add(K.layers.Dropout(0.5))\n",
    "    model.add(K.layers.BatchNormalization())\n",
    "    model.add(K.layers.Dense(128, activation=\"relu\"))\n",
    "    model.add(K.layers.Dropout(0.5))\n",
    "    model.add(K.layers.BatchNormalization())\n",
    "    model.add(K.layers.Dense(64, activation=\"relu\"))\n",
    "    model.add(K.layers.Dropout(0.5))\n",
    "    model.add(K.layers.BatchNormalization())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecd395b-1535-40c2-9fcf-cd18d8880eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "nntrain = int(0.7*nn)\n",
    "np.random.seed(42)\n",
    "indices = np.random.permutation(nn)\n",
    "train_idx, test_idx = indices[:nntrain], indices[nntrain:]\n",
    "mymodel = PredictionModel(X=X, Y=Y, idx=[train_idx,test_idx])\n",
    "\n",
    "# train model\n",
    "for i in range(3):\n",
    "    mymodel.fit(index=i, model=initializeModel(), epochs=5, save=True, save_location=SAVEPATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6527b5ae",
   "metadata": {},
   "source": [
    "## Evaluate the Model\n",
    "\n",
    "We now evaluate the model on the test data. To do this, let's first load up that data and structure it similarly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ab27c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load labels data\n",
    "TESTPATH = './test/'\n",
    "df_test = pd.read_csv(TESTPATH+'labels.csv')\n",
    "\n",
    "# Convert labels to np array\n",
    "print(\"Converting test labels to np array\")\n",
    "testY = []\n",
    "for i in range(3):\n",
    "    lab = lbs[i].fit_transform(df_test[cat[i]])\n",
    "    if lab.shape[1]==1:\n",
    "        testY.append(np.hstack((1-lab,lab)))\n",
    "    else:\n",
    "        testY.append(lab)\n",
    "        \n",
    "# load and convert images into np array\n",
    "print(\"Loading test images\")\n",
    "nt = df_test.shape[0]\n",
    "all_imgs = [image.load_img(TESTPATH+df_test.iloc[i]['name'], target_size=(length,width)) for i in range(nt)]\n",
    "\n",
    "print(\"Converting test images to np array\")\n",
    "testX = np.empty([nt, length, width, 3], dtype=float)\n",
    "for i in range(nt):\n",
    "    testX[i,:] = image.img_to_array(all_imgs[i])\n",
    "testX = K.applications.resnet50.preprocess_input(testX)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07ee94e",
   "metadata": {},
   "source": [
    "We then obtain predicted labels for skin tone, gender, and age as a list of lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bf106302",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = mymodel.predict(testX)\n",
    "predY = [[np.argmax(pred[i][j,:]) for j in range(nt)] for i in range(3)]\n",
    "predLabels = [[lbs[i].classes_[j] for j in predY[i]] for i in range(3)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0cf73d",
   "metadata": {},
   "source": [
    "Finally, we calculate the label-wise accuracy and disparity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "081d5cdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': {'skin_tone': 0.25766666666666665,\n",
       "  'gender': 0.7966666666666666,\n",
       "  'age': 0.5783333333333334},\n",
       " 'disparity': {'skin_tone': 0.5514223194748359,\n",
       "  'gender': 0.1493182689960596,\n",
       "  'age': 0.7802908824936}}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate accuracy\n",
    "acc = {}\n",
    "for i in range(3):\n",
    "    icat = cat[i]\n",
    "    iacc = accuracy_score(df_test[cat[i]], predLabels[i])\n",
    "    acc[icat] = iacc\n",
    "\n",
    "# calculate disparity\n",
    "def disparity_score(ytrue, ypred):\n",
    "    cm = confusion_matrix(ytrue,ypred)\n",
    "    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    all_acc = list(cm.diagonal())\n",
    "    return max(all_acc) - min(all_acc)\n",
    "\n",
    "disp = {}\n",
    "for i in range(3):\n",
    "    icat = cat[i]\n",
    "    idisp = disparity_score(df_test[cat[i]], predLabels[i])\n",
    "    disp[icat] = idisp\n",
    "disp\n",
    "\n",
    "results = {'accuracy': acc, 'disparity': disp}\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20b0fcd",
   "metadata": {},
   "source": [
    "# Score Model and Prepare Submission\n",
    "\n",
    "Based on the above metric, we now calculate the score to evaluate your submission. This score will be displayed in your public leaderboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "db5b9086",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'submission_name': '8-Bit Bias Bounty Baseline',\n",
       " 'score': 4.705572543130653,\n",
       " 'metrics': {'accuracy': {'skin_tone': 0.25766666666666665,\n",
       "   'gender': 0.7966666666666666,\n",
       "   'age': 0.5783333333333334},\n",
       "  'disparity': {'skin_tone': 0.5514223194748359,\n",
       "   'gender': 0.1493182689960596,\n",
       "   'age': 0.7802908824936}}}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getScore(results):\n",
    "    acc = results['accuracy']\n",
    "    disp = results['disparity']\n",
    "    ad = 2*acc['gender']*(1-disp['gender']) + 4*acc['age']*(1-disp['age']**2) + 10*acc['skin_tone']*(1-disp['skin_tone']**5)\n",
    "    return ad\n",
    "\n",
    "title = '8-Bit Bias Bounty Baseline'\n",
    "    \n",
    "submission = {\n",
    "    'submission_name': title,\n",
    "    'score': getScore(results),\n",
    "    'metrics': results\n",
    "}\n",
    "submission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19070a4e",
   "metadata": {},
   "source": [
    "Finally, let's export this as a json file to upload as part of filling out your [submission form](https://docs.google.com/forms/d/e/1FAIpQLSfwqtVkJBVRP6TnFp7vHbbH8SlwKZJFIjvGQy7TyYFc8HR1hw/viewform)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34c30e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"baseline_score.json\", \"w\") as f:\n",
    "    json.dump(submission, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-2:712779665605:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "1910b6696973d15bf216d34a2b579ebfef615f9fbc990caf22091bbcd689422a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
